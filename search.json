[
  {
    "objectID": "docs/notebooks/networks_author.html",
    "href": "docs/notebooks/networks_author.html",
    "title": "Systematic literature review",
    "section": "",
    "text": "This was the original document metadata, which I am over-riding above, just to get things working in this documentation site.",
    "crumbs": [
      "Systematic literature review"
    ]
  },
  {
    "objectID": "docs/notebooks/networks_author.html#purpose",
    "href": "docs/notebooks/networks_author.html#purpose",
    "title": "Systematic literature review",
    "section": "1 Purpose",
    "text": "1 Purpose\n\ncowsay::say(\"After researching the articles and references by making graphs to\nbetter visualize the structure of the research. We want to focus\nhere on the authors, trying to understand how communities evolve over time.\")",
    "crumbs": [
      "Systematic literature review"
    ]
  },
  {
    "objectID": "docs/notebooks/networks_author.html#libraries-and-preparing-data",
    "href": "docs/notebooks/networks_author.html#libraries-and-preparing-data",
    "title": "Systematic literature review",
    "section": "2 Libraries and preparing data",
    "text": "2 Libraries and preparing data\n\n2.1 Summary of the authors data\n\n```{r}\n#| label: summary-authors-data\n#| column: screen-inset-right\n\nlibrary(tidyverse)\nlibrary(reactable)\nlibrary(gt)\nlibrary(skimr)\nlibrary(plotly)\nlibrary(reticulate)\nlibrary(patchwork)\n\n\n\nlist_articles &lt;- read_csv2(\"nlp_full_data_final_18-08-2023.csv\") %&gt;%\n  mutate(marketing = as.logical(marketing)) %&gt;%\n  mutate(authid = as.character(authid)) %&gt;%\n  mutate(afid = as.character(afid)) %&gt;%\n  mutate(entry_number = as.character(entry_number)) %&gt;%\n  mutate(source_id = as.character(source_id)) %&gt;%\n  mutate(article_number = as.character(article_number)) %&gt;%\n  mutate(openaccess = as.logical(openaccess))\n\nskim(list_articles) #%&gt;%\n  #filter(!skim_type %in% c(\"logical\"))\n```\n\n\n\n2.2 Check name of authors\nWe need to check if there are more than one unique authorname per authid. If so, we need to change the different names of author to the same name in order to have the exact same node per author later in the network.\n\ntest &lt;- list_articles %&gt;%\n  group_by(authid) %&gt;%\n  select(authid, authname, entry_number) %&gt;%\n  mutate(n = n())\n\nresult &lt;- test %&gt;%\n  group_by(authid) %&gt;%\n  filter(n_distinct(authname) &gt; 1) %&gt;%\n  distinct(authid, .keep_all = TRUE)\n\nresult %&gt;% reactable()\n\nnumber_duplicates &lt;- nrow(result)\n\ncat(\"There are \", number_duplicates, \" authors registered with different names.\")\n\n\n\n2.3 Correct the duplicate names\nLet’s correct that by using one property of the distinct function: the .keep_all = TRUE parameter. It keeps the first occurrence of each group, which is the first row encountered for each unique combination of authid and authname. It will be faster than manually changing the name of each author.\n\n# Merge list_articles with result on the authid column\nmerged_df &lt;- left_join(list_articles, result, by = \"authid\")\n\n# Replace authname values in list_articles with those from result\nlist_articles$authname &lt;- ifelse(!is.na(merged_df$authname.y), merged_df$authname.y, list_articles$authname)\n\n# Keep only marketing articles and filter \"Erratum\" type of publications (=correction)\nlist_articles &lt;- list_articles %&gt;% \n  filter(marketing == 1) %&gt;%\n  filter(subtypeDescription != \"Erratum\")\n\n\ncat(\"There are\", n_distinct(list_articles$entry_number), \"articles and\", n_distinct(list_articles$authname), \"authors overall in the data.\")\n\n# Write the updated dataframe to a CSV file \nwrite_csv2(list_articles, \"nlp_full_data_final_unique_author_names.csv\")\n\nIt is now done. We can check again if there are more than one unique authorname per authid.\n\n\n2.4 Verification of duplicate names\n\ntest &lt;- list_articles %&gt;%\n  group_by(authid) %&gt;%\n  select(authid, authname, entry_number) %&gt;%\n  mutate(n = n())\n\nresult &lt;- test %&gt;%\n  group_by(authid) %&gt;%\n  filter(n_distinct(authname) &gt; 1) %&gt;%\n  distinct(authid, .keep_all = TRUE) %&gt;%\n  relocate(entry_number)\n\nresult %&gt;% reactable()\n\nIt’s alright, we can now continue on constructing the data frames for the networks.",
    "crumbs": [
      "Systematic literature review"
    ]
  },
  {
    "objectID": "docs/notebooks/networks_author.html#co-authorship-networks",
    "href": "docs/notebooks/networks_author.html#co-authorship-networks",
    "title": "Systematic literature review",
    "section": "3 Co-authorship networks",
    "text": "3 Co-authorship networks\n\n#G = nx.from_pandas_edgelist(network_data_2022_2023, 'authname1', 'authname2', edge_attr='value', create_using=nx.Graph())\n\n\n3.1 Network basic visualization\nThis is a basic visualization done with the NetworkX library and matplotlib.\n\n```{python}\n#| label: network-visualization\n#| fig-cap: Visualization of the co-authorship network for the 2022-2023 period, created using NetworkX and matplotlib.\n#| fig-align: center\n\n#plt.figure(figsize=(20,20))\n#pos = nx.kamada_kawai_layout(G)\n\n#nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos=pos)\n```\n\n\n\n\n\n\n\nCaution\n\n\n\nThis is not interactive and the result is not enlightening at all. We then decide to use Pyvis to create an interactive visualization.\n\n\n\n\n3.2 Network visualization with Pyvis\n\n\n\n\n\n\nTip\n\n\n\nPyvis enables us to create interactive visualizations and modify the network layout in real time with the net.show_buttons(filter_=['physics']) command. This button then generates options to include in our code by using the net.set_options() function. More information here.\n\n\n\n```{python}\n#| label: network-visualization-pyvis\n#| output: false\n\n#from pyvis.network import Network\n\n\n#net = Network(notebook=True, cdn_resources='remote', width=1500, height=900, bgcolor=\"white\", font_color=\"black\")\n#net.show_buttons(filter_=['physics'])\n#net.set_options(\"\"\"\n#const options = {\n#  \"physics\": {\n#    \"forceAtlas2Based\": {\n#      \"gravitationalConstant\": -13,\n#      \"centralGravity\": 0.015,\n#      \"springLength\": 70\n#    },\n#    \"minVelocity\": 0.75,\n#    \"solver\": \"forceAtlas2Based\"\n#  }\n#}\n#\"\"\")\n\n#node_degree = dict(G.degree)\n\n## Some values for nodes\n# Multiply node sizes by two\n#node_degree_doubled = {node: 2 * degree for node, degree in node_degree.items()}\n#node_degree_centrality = nx.degree_centrality(G)\n#node_degree_betweenness = nx.betweenness_centrality(G)\n#node_degree_closeness = nx.closeness_centrality(G)\n#node_degree_constraint = nx.constraint(G)\n\n\n# Set the node attributes with the doubled sizes\n#nx.set_node_attributes(G, node_degree_doubled, 'size')\n#nx.set_node_attributes(G, node_degree_centrality, 'centrality')\n#nx.set_node_attributes(G, node_degree_betweenness, 'betweenness')\n#nx.set_node_attributes(G, node_degree_closeness, 'closeness')\n#nx.set_node_attributes(G, node_degree_constraint, 'constraint')\n##nx.set_node_attributes(G, affilauthor_2022_2023, 'affiliation')\n#nx.set_node_attributes(G, countryauthor_2022_2023, 'country')\n#nx.set_node_attributes(G, citations_2022_2023, 'citations')\n#nx.set_node_attributes(G, article_2022_2023, 'title')\n#nx.set_node_attributes(G, journal_2022_2023, 'journal')\n\n#listnodes = net.nodes\n#net.nodes.__getitem__(1)\n\n#listnodes = net.nodes\n\n#net.from_nx(G)\n\n#net.show(\"networks/authors/network_2022_2023_pyvis.html\")\n```\n\n\n\n\n3.3 Detect communities with Louvain’s algorithm\n\n```{python}\n#| label: community-detection-louvain\n#| output: false\n\n#import community as community_louvain\n\n# Compute the best partition\n#communities = community_louvain.best_partition(G)\n\n#dftest = pd.DataFrame(list(communities.items()), columns=['authname', 'community'])\n\n#nx.set_node_attributes(G, communities, 'group')\n```\n\n\n```{python}\n#| label: network-visualization-pyvis-louvain\n#| output: false\n#com_net = Network(notebook=True, cdn_resources='remote', width=1500, height=900, bgcolor=\"white\", font_color=\"black\")\n#com_net.set_options(\"\"\"\n#const options = {\n#  \"physics\": {\n#    \"forceAtlas2Based\": {\n#      \"gravitationalConstant\": -13,\n#      \"centralGravity\": 0.015,\n#      \"springLength\": 50\n#    },\n#    \"minVelocity\": 0.75,\n#    \"solver\": \"forceAtlas2Based\"\n#  }\n#}\n#\"\"\")\n\n#com_net.from_nx(G)\n#com_net.show(\"networks/authors/network_2022_2023_louvain_pyvis.html\")\n```\n\n\n\n\n3.4 Network visualization with ipysigma [@plique:hal-03903518v1]\n\n3.4.1 A function to graph them all\n\n\n\n\n\n\n\n\n# Constants\nCOLUMNS_TO_COLLECT = [\n    'affilname', 'affiliation_country', 'dc:title', 'prism:publicationName',\n    'subtypeDescription', 'year', 'citedby_count', 'subjects_area', 'authkeywords'\n]\n\ndef get_author_info(filtered_articles, columns):\n    \"\"\"\n    Given a DataFrame of filtered articles and a list of column names,\n    this function collects author information and returns it as a dictionary.\n    \"\"\"\n    author_info = {col: {} for col in columns}\n    author_info[\"citations\"] = {}\n\n    for _, row in filtered_articles.iterrows():\n        author_name = row['authname']\n\n        if pd.notna(row['citedby_count']):\n            author_info[\"citations\"][author_name] = author_info[\"citations\"].get(author_name, 0) + row['citedby_count']\n\n        for col in columns:\n            value = row[col]\n            if pd.notna(value):\n                value = str(value).strip()\n                if author_name in author_info[col]:\n                    if value not in author_info[col][author_name]:\n                        author_info[col][author_name] += \" | \" + value\n                else:\n                    author_info[col][author_name] = value\n\n    return author_info\n\ndef sigma_graph(dataframe, start_year, end_year):\n    \"\"\"\n    Creates a graph representing author collaborations based on a given DataFrame of articles.\n    Filters the articles based on the given start and end years.\n    \"\"\"\n    # Error handling\n    if dataframe.empty:\n        print(\"The DataFrame is empty.\")\n        return None, None\n\n    for column in COLUMNS_TO_COLLECT:\n        if column not in dataframe.columns:\n            print(f\"The DataFrame is missing the column: {column}\")\n            return None, None\n\n    list_articles = dataframe\n    filtered_articles = list_articles[(list_articles['year'] &gt;= start_year) & (list_articles['year'] &lt;= end_year)]\n\n    author_pairs = []\n    grouped = filtered_articles.groupby('entry_number')[['authid', 'authname']].agg(list).reset_index()\n\n    for _, row in grouped.iterrows():\n        entry_number = row['entry_number']\n        authors = row['authid']\n        authnames = row['authname']\n\n        if len(authors) == 1:\n            author_pairs.append((entry_number, authors[0], authors[0], authnames[0], authnames[0]))\n        elif len(authors) &gt; 1:\n            author_combinations = list(combinations(range(len(authors)), 2))\n            for i, j in author_combinations:\n                author_pairs.append((entry_number, authors[i], authors[j], authnames[i], authnames[j]))\n\n    result_df = pd.DataFrame(author_pairs, columns=['entry_number', 'authid1', 'authid2', 'authname1', 'authname2'])\n\n    collaboration_df = result_df[[\"authname1\", \"authname2\"]]\n    collaboration_df = pd.DataFrame(np.sort(collaboration_df.values, axis=1), columns=collaboration_df.columns)\n    collaboration_df['value'] = 1\n    collaboration_df = collaboration_df.groupby([\"authname1\", \"authname2\"], sort=False, as_index=False).sum()\n\n    G = nx.from_pandas_edgelist(collaboration_df, 'authname1', 'authname2', edge_attr='value', create_using=nx.Graph())\n\n    for u, v in G.edges:\n        G[u][v][\"color\"] = \"#7D7C7C\"\n\n    for index, row in collaboration_df.iterrows():\n        G.add_edge(row['authname1'], row['authname2'], weight=row['value'])\n\n    metrics = {\n        'centrality': nx.degree_centrality,\n        'betweenness': nx.betweenness_centrality,\n        'closeness': nx.closeness_centrality,\n        'eigenvector_centrality': partial(nx.eigenvector_centrality, max_iter=1000),\n        'burt_constraint_weighted': partial(nx.constraint, weight=\"value\"),\n        'burt_constraint_unweighted': nx.constraint\n    }\n\n    for attr, func in metrics.items():\n        nx.set_node_attributes(G, func(G), attr)\n\n    author_info = get_author_info(filtered_articles, COLUMNS_TO_COLLECT)\n\n    for col in COLUMNS_TO_COLLECT:\n        nx.set_node_attributes(G, author_info[col], col)\n\n    nx.set_node_attributes(G, author_info['citations'], 'citations')\n\n    # Compute the inverse burt constraint to plot the lowest values as the biggest nodes\n    # (= authors that are the less constrained in their network =&gt; multiple probable collaborations)\n    for node in G.nodes:\n        # Check if the 'burt_constraint_weighted' metric exists for the node\n        if 'burt_constraint_weighted' in G.nodes[node]:\n            burt_score = G.nodes[node]['burt_constraint_weighted']\n            # Calculate the inverse, avoiding division by zero\n            G.nodes[node]['inverse_burt_weighted'] = 1 / burt_score if burt_score != 0 else 0\n        else:\n            # Handle the case where the 'burt_constraint_weighted' metric is not available for this node\n            # For example, by setting the value to None or a default value\n            G.nodes[node]['inverse_burt_weighted'] = None  # or another default value\n            \n    # Compute Louvain commmunities\n    partition = community_louvain.best_partition(G)\n    \n    for node, comm_number in partition.items():\n      G.nodes[node]['community'] = comm_number\n      \n    # Color the graph using the greedy coloring algorithm with the 'largest_first' strategy\n    colors = nx.greedy_color(G, strategy='largest_first', interchange=False)\n    \n    # Set the computed colors as an attribute to each node in the graph\n    nx.set_node_attributes(G, colors, 'color')\n    # Now, each node in the graph G has an attribute 'color' that corresponds to the color assigned by the greedy_color function\n\n    \n    data_for_df = []\n    for node in G.nodes(data=True):\n        # `node` is a tuple (node_name, attributes_dict)\n        node_data = node[1]  # Extracting the attributes dictionary\n        node_data['Node'] = node[0]  # Adding the node name as an attribute\n    \n        # Adding the attributes dictionary of this node to the list\n        data_for_df.append(node_data)\n    \n    # Creating a DataFrame from the list of dictionaries\n    df_nodes = pd.DataFrame(data_for_df)\n\n    Sigma.write_html(G,\n                 default_edge_type       = \"curve\",                                                     # Default edge type\n                 clickable_edges         = True,                                                        # Clickable edges\n                 edge_size               = \"value\",                                                     # Set edge size\n                 fullscreen              = True,                                                        # Display in fullscreen\n                 label_density           = 3,                                                           # Label density (= increase to have more labels appear at normal zoom level)\n                 label_font              = \"Helvetica Neue\",                                            # Label font\n                 max_categorical_colors  = 10,                                                          # Max categorical colors\n                 node_border_color_from  = 'node',                                                      # Node border color from node attribute\n                 node_color              = \"community\",                                                 # Set node colors\n                 node_label_size         = \"inverse_burt_weighted\",                                                 # Node label size\n                 #node_label_size_range   = (12, 36),                                                    # Node label size range\n                 node_label_size_range   = (12, 36),                                                    # Node label size range\n                 #node_metrics            = {\"community\": {\"name\": \"louvain\", \"resolution\": 1}},         # Specify node metrics\n                 node_size               = \"inverse_burt_weighted\",                                                 # Node size\n                 #node_size_range         = (3, 30),                                                     # Node size range\n                 node_size_range         = (2, 20),                                                     # Node size range\n                 path                    = f\"networks/authors/{start_year}_{end_year}_sigma_v2_burt.html\",   # Output file path\n                 start_layout            = 3,                                                           # Start layout algorithm\n                 #node_border_color      = \"black\",                                                     # Node border color\n                 #edge_color             = \"#7D7C7C\"                                                    # Edge color\n                 # node_label_color      = \"community\"                                                  # Node label color\n                 )\n    \n\n    return G, df_nodes\n\n\n\n3.4.2 Co-authorship network for the 2021-2023 period (click here for fullscreen)\n\nG_2021_2023, df_2021_2023 = sigma_graph(list_articles, 2021, 2023)\n\n\n#plot distribution of communities\n\ndf_2021_2023 &lt;- py$df_2021_2023\n\n#select top 5 authors with lowest burts constraint\ndf_2021_2023 %&gt;%\n  arrange(inverse_burt_weighted)\n  \ntop5communities &lt;- df_2021_2023 %&gt;%\n  group_by(community) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;%\n  head(5) %&gt;%\n  select(community)\n\ndf_2021_2023 %&gt;%\n  filter(community %in% top5communities$community)\n\n\n\n\n3.4.3 Co-authorship network for the 2018-2022 period (click here for fullscreen)\n\nG_2018_2022, df_2018_2022 = sigma_graph(list_articles, 2018, 2022)\ncolors = nx.greedy_color(G_2018_2022, strategy='largest_first', interchange=False)\ncolorsdf = pd.DataFrame.from_dict(colors, orient='index', columns=['color'])\n#plot distribution of colors\ncolorsdf['color'].value_counts().plot(kind='bar', figsize=(10, 6), rot=0, title=\"Distribution of colors in the 2018-2022 network\")\nplt.show()\n\n#print(\"The density of the graph is {}\".format(nx.density(G_2018_2021))\n\ncommunities = community_louvain.best_partition(G_2018_2022)\nprint(\"There are {} communities in the 2018-2022 network\".format(len(set(communities.values()))))\n\n\n\n\n3.4.4 Co-authorship network for the 2015-2019 period (click here for fullscreen)\n\nG_2015_2019, df_2015_2019 = sigma_graph(list_articles, 2015, 2019)\ncolors = nx.greedy_color(G_2015_2019, strategy='largest_first', interchange=False)\ncolorsdf = pd.DataFrame.from_dict(colors, orient='index', columns=['color'])\n#plot distribution of colors\ncolorsdf['color'].value_counts().plot(kind='bar', figsize=(10, 6), rot=0, title=\"Distribution of colors in the 2015-2019 network\")\nplt.show()\n\n#print(\"The density of the graph is {}\".format(nx.density(G_2013_2017))\n\ncommunities = community_louvain.best_partition(G_2015_2019)\nprint(\"There are {} communities in the 2015-2019 network\".format(len(set(communities.values()))))\n\n\n\n\n3.4.5 Co-authorship network for the 2014-2019 period (click here for fullscreen)\n\nG_2014_2019, df_2014_2019 = sigma_graph(list_articles, 2014, 2019)\ncolors = nx.greedy_color(G_2014_2019, strategy='largest_first', interchange=False)\ncolorsdf = pd.DataFrame.from_dict(colors, orient='index', columns=['color'])\n#plot distribution of colors\ncolorsdf['color'].value_counts().plot(kind='bar', figsize=(10, 6), rot=0, title=\"Distribution of colors in the 2014-2019 network\")\nplt.show()\n\ncommunities = community_louvain.best_partition(G_2014_2019)\nprint(\"There are {} communities in the 2014-2019 network\".format(len(set(communities.values()))))\n\n\n\n\n3.4.6 Co-authorship network until 2015 (click here for fullscreen)\n\nG_0_2015, df_0_2015 = sigma_graph(list_articles, 0, 2015)\ncolors = nx.greedy_color(G_0_2015, strategy='largest_first', interchange=False)\ncolorsdf = pd.DataFrame.from_dict(colors, orient='index', columns=['color'])\n#plot distribution of colors\ncolorsdf['color'].value_counts().plot(kind='bar', figsize=(10, 6), rot=0, title=\"Distribution of colors in the 0-2015 network\")\nplt.show()\n\n\ncommunities = community_louvain.best_partition(G_0_2015)\nprint(\"There are {} communities in the 0-2015 network\".format(len(set(communities.values()))))\n\n\n\n\n\n3.5 An interesting metric: the graph density\nThe graph density is the ratio of the number of edges to the maximum number of possible edges. It is a measure of the proportion of edges present in a graph. A graph with a high density has a large number of edges compared to the number of nodes. A graph with a low density has a small number of edges compared to the number of nodes.\nA more formal definition is given here by the following formulas:\n\nFor undirected graphs:\n\n\\[\n\\begin{equation}d=\\frac{2 m}{n(n-1)}\\end{equation}\n\\]\n\nFor directed graphs:\n\n\\[\n\\begin{equation}d=\\frac{m}{n(n-1)}\\end{equation}\n\\]\nwhere \\(n\\) is the number of nodes and \\(m\\) is the number of edges in the graph.\nFrom an interpretation standpoint, we can appreciate the density in the graphs bellow as follows:\n\n\n\n\n\n\n\n\\(d\\)\nInterpretation\n\n\n\n\nClose to \\(0\\)\n\nThe collaborative relationships among authors are sparse:\nAuthors have limited connections with each other outside of their community.\nScientific papers are primarily the work of individual authors or small isolated groups.\n\n\n\nClose to \\(1\\)\n\nAuthors frequently collaborate with one another, leading to a web of interconnected scientific collaborations.\nScientific papers often involve contributions from multiple authors, reflecting a high level of teamwork and interdisciplinary research.\nCollaborations are a significant aspect of the research process in this marketing field, and authors actively seek out opportunities to work together.\nThe network of collaborations is well-established and robust, facilitating the exchange of ideas and the advancement of scientific knowledge.\n\n\n\n\n\n3.5.1 Evolution of the graphs’ density\n\ndef average_degree(G):\n    # Calculate the sum of degrees of all nodes\n    total_degree = sum(dict(G.degree()).values())\n    \n    # Divide by the number of nodes to get the average degree\n    avg_degree = total_degree / G.number_of_nodes()\n    \n    return avg_degree\n  \ndef linear_density(G):\n    if len(G.nodes()) == 0:  # Pour éviter une division par zéro\n        return 0\n    return len(G.edges()) / len(G.nodes())\n  \n# Create a dataframe with the density of each graph\n#density_df = pd.DataFrame({\n    #'period': ['before-2013', '2013-2017', '2018-2021', '2022-2023'],\n    #'density': [\n      #nx.density(G_before_2013),\n      #nx.density(G_2013_2017),\n      #nx.density(G_2018_2021),\n      #nx.density(G_2022_2023)\n      #],\n    #'average_degree': [\n        #average_degree(G_before_2013),\n        #average_degree(G_2013_2017),\n        #average_degree(G_2018_2021),\n        #average_degree(G_2022_2023)\n    #],\n    #'linear_density': [\n        #linear_density(G_before_2013),\n        #linear_density(G_2013_2017),\n        #linear_density(G_2018_2021),\n        #linear_density(G_2022_2023)\n    #]\n#})\n\n\n#library(Hmisc)\n\n# Load the 'density_df' dataframe from Python using reticulate\n#testtransfer &lt;- py$density_df\n\n# Specify the order of categories for the 'period' column\n#testtransfer$period &lt;- factor(testtransfer$period, levels = c('before-2013', '2013-2017', '2018-2021', '2022-2023'))\n\n# Use the 'gt()' function to display the dataframe\n#testtransfer %&gt;%\n  #rename_all(Hmisc::capitalize) %&gt;%\n  #gt() %&gt;%\n  #tab_style(\n    #style = cell_text(weight = \"bold\", align = \"center\"),\n    #locations = cells_column_labels()\n    #)\n        \n \n# Create the Plotly graph\n#fig &lt;- plot_ly(testtransfer, x = ~period, y = ~density, type = 'scatter', mode = 'lines+markers', \n               #text = ~paste(\"Period=\", period, \"&lt;br&gt;Density=\", density), hoverinfo = \"text\")\n\n# Show the graph\n#fig &lt;- fig %&gt;% layout(template = \"plotly_white\")\n\n#fig",
    "crumbs": [
      "Systematic literature review"
    ]
  },
  {
    "objectID": "docs/notebooks/networks_author.html#graph-density-of-references",
    "href": "docs/notebooks/networks_author.html#graph-density-of-references",
    "title": "Systematic literature review",
    "section": "4 Graph density of references",
    "text": "4 Graph density of references\n\n# Create a dataframe with the density of each graph\n#density_df_references = pd.DataFrame({\n    #'period': ['before-2013', '2013-2017', '2018-2021', '2022-2023', 'overall'],\n    #'density': [\n        #nx.density(G_before_2013_references), \n        #nx.density(G_2013_2017_references), \n        #nx.density(G_2018_2021_references), \n        #nx.density(G_2022_2023_references),\n        #nx.density(G_overall_references)\n    #],\n    #'average_degree': [\n        #average_degree(G_before_2013_references),\n        #average_degree(G_2013_2017_references),\n        #average_degree(G_2018_2021_references),\n        #average_degree(G_2022_2023_references),\n        #average_degree(G_overall_references)\n    #],\n    #'linear_density': [\n    #linear_density(G_before_2013_references),\n    #linear_density(G_2013_2017_references),\n    #linear_density(G_2018_2021_references),\n    #linear_density(G_2022_2023_references),\n    #linear_density(G_overall_references)\n    #]\n#})\n\n```{r}\n#| label: citations-graph-density-comparison\n#| fig.cap: Comparison of network densities and average degree of nodes over time\n#| column: body-outset\n\n# Density plot\ndensity_plot &lt;- ggplot() +\n  geom_line(data = py$density_df, aes(x = period, y = density, colour = \"Collaboration Density\", group=1, text = paste(\"Period:\", period, \"&lt;br&gt;Density:\", density)), linewidth=1) +\n  geom_line(data = py$density_df_references %&gt;% filter(period != \"overall\"), aes(x = period, y = density, colour = \"References Density\", group=1, text = paste(\"Period:\", period, \"&lt;br&gt;Density:\", density)), linewidth=1) +\n  scale_y_continuous(name = \"Graphs Density\") +\n  scale_x_discrete(limits = c(\"before-2013\", \"2013-2017\", \"2018-2021\", \"2022-2023\")) +\n  xlab(\"Period\") +\n  ggtitle(\"Comparison of Network Density Over Time\") +\n  theme_minimal()\n\n# Linear density plot (m/n)\nlinear_density_plot &lt;- ggplot() +\n  geom_line(data = py$density_df, aes(x = period, y = linear_density, colour = \"Collaboration Linear Density\", group=1, text = paste(\"Period:\", period, \"&lt;br&gt;Linear Density:\", linear_density)), linewidth=1) + # Adjusted for the new column \"linear_density\"\n  geom_line(data = py$density_df_references %&gt;% filter(period != \"overall\"), aes(x = period, y = linear_density, colour = \"References Linear Density\", group=1, text = paste(\"Period:\", period, \"&lt;br&gt;Linear Density:\", linear_density)), linewidth=1) + # Adjusted for the new column \"linear_density\"\n  scale_y_continuous(name = \"Graphs Linear Density\") +\n  scale_x_discrete(limits = c(\"before-2013\", \"2013-2017\", \"2018-2021\", \"2022-2023\")) +\n  xlab(\"Period\") +\n  ggtitle(\"Comparison of Network Linear Density Over Time\") +\n  theme_minimal()\n\n# Create average degree plot\navg_degree_plot &lt;- ggplot() +\n  geom_line(data = py$density_df, aes(x = period, y = average_degree, colour = \"Collaboration Average Degree\", group=1, text = paste(\"Period:\", period, \"&lt;br&gt;Average Degree:\", average_degree)), linewidth=1) + # Corrected here\n  geom_line(data = py$density_df_references %&gt;% filter(period != \"overall\"), aes(x = period, y = average_degree, colour = \"References Average Degree\", group=1, text = paste(\"Period:\", period, \"&lt;br&gt;Average Degree:\", average_degree)), linewidth=1) +\n  scale_y_continuous(name = \"Nodes Average Degree\") +\n  scale_x_discrete(limits = c(\"before-2013\", \"2013-2017\", \"2018-2021\", \"2022-2023\")) +\n  xlab(\"Period\") +\n  ggtitle(\"Comparison of Network Average Degree Over Time\") +\n  theme_minimal()\n\n# Combine density and average degree plots\ndensity_plot / linear_density_plot / avg_degree_plot\n\nggsave(\"images/citations-graph-density-comparison.png\", width=270, height=180, units=\"cm\", dpi=300)\n\n```",
    "crumbs": [
      "Systematic literature review"
    ]
  },
  {
    "objectID": "docs/demo/kuzu-quarto-live.html",
    "href": "docs/demo/kuzu-quarto-live.html",
    "title": "Quarto graph experiment - DylanShang version",
    "section": "",
    "text": "What is in the data folder?\n\n\n\n\n\n\n\n\nGenerate self-contained graph\n\n\n\n\n\n\n\n\nTry to Connect to a pre-existing kuzu-test database\n\n\n\n\n\n\nWould expect to get returned something like:\n┌──────────────┬────────┬───────────────┬─────────┐\n│ name         │ type   │ database name │ comment │\n│ STRING       │ STRING │ STRING        │ STRING  │\n├──────────────┼────────┼───────────────┼─────────┤\n│ has_source   │ REL    │ local(kuzu)   │         │\n│ has_category │ REL    │ local(kuzu)   │         │\n│ Author       │ NODE   │ local(kuzu)   │         │\n│ Category     │ NODE   │ local(kuzu)   │         │\n│ authored_by  │ REL    │ local(kuzu)   │         │\n│ Source       │ NODE   │ local(kuzu)   │         │\n│ Document     │ NODE   │ local(kuzu)   │         │\n└──────────────┴────────┴───────────────┴─────────┘\nInstead, I’m getting back this:\n./data/kuzu-test\nname|type|database name|comment\n\nget_as_df\nWhich would suggest a connection is not being made to the kuzu database.",
    "crumbs": [
      "Quarto graph experiment - DylanShang version"
    ]
  },
  {
    "objectID": "docs/notebooks/General_Kùzu_Demo.html",
    "href": "docs/notebooks/General_Kùzu_Demo.html",
    "title": "General Kuzu Demo",
    "section": "",
    "text": "Download and install\nInstall Kùzu Python API with pip.\n\n%%capture\n!pip install kuzu==0.6.0\n\n\n\nDataset\nIn this demo notebook, we use the following graph database that consists of User and City nodes, Follows relationships between Users, and LivesIn relationships between Users and Cities.\n\n\n\nrunning-example.png\n\n\nWe first create this test dataset in pandas and export it as CSV.\n\nimport pandas as pd\n\nuser_data = [\n    { \"name\": \"Adam\", \"age\": 30 },\n    { \"name\": \"Karissa\", \"age\": 40 },\n    { \"name\": \"Zhang\", \"age\": 50 },\n    { \"name\": \"Noura\", \"age\": 25 },\n]\n\nfollows_data = [\n    { \"from\": \"Adam\", \"to\": \"Karissa\", \"since\": 2020 },\n    { \"from\": \"Adam\", \"to\": \"Zhang\", \"since\": 2020 },\n    { \"from\": \"Karissa\", \"to\": \"Zhang\", \"since\": 2021 },\n    { \"from\": \"Zhang\", \"to\": \"Noura\", \"since\": 2022 }\n]\n\ncity_data = [\n    { \"name\": \"Waterloo\", \"population\": 150000 },\n    { \"name\": \"Kitchener\", \"population\": 200000 },\n    { \"name\": \"Guelph\", \"population\": 75000 }\n]\n\nlives_in_data = [\n    { \"from\": \"Adam\", \"to\": \"Waterloo\" },\n    { \"from\": \"Karissa\", \"to\": \"Waterloo\" },\n    { \"from\": \"Zhang\", \"to\": \"Kitchener\" },\n    { \"from\": \"Noura\", \"to\": \"Guelph\" },\n]\n\nuser_df = pd.DataFrame(user_data)\nfollows_df = pd.DataFrame(follows_data)\ncity_df = pd.DataFrame(city_data)\nlives_in_df = pd.DataFrame(lives_in_data)\n\n\nuser_df\n\n\n  \n    \n\n\n\n\n\n\nname\nage\n\n\n\n\n0\nAdam\n30\n\n\n1\nKarissa\n40\n\n\n2\nZhang\n50\n\n\n3\nNoura\n25\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nfollows_df\n\n\n  \n    \n\n\n\n\n\n\nfrom\nto\nsince\n\n\n\n\n0\nAdam\nKarissa\n2020\n\n\n1\nAdam\nZhang\n2020\n\n\n2\nKarissa\nZhang\n2021\n\n\n3\nZhang\nNoura\n2022\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ncity_df\n\n\n  \n    \n\n\n\n\n\n\nname\npopulation\n\n\n\n\n0\nWaterloo\n150000\n\n\n1\nKitchener\n200000\n\n\n2\nGuelph\n75000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nlives_in_df\n\n\n  \n    \n\n\n\n\n\n\nfrom\nto\n\n\n\n\n0\nAdam\nWaterloo\n\n\n1\nKarissa\nWaterloo\n\n\n2\nZhang\nKitchener\n\n\n3\nNoura\nGuelph\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nuser_df.to_csv(\"user.csv\", header=False, index=False)\nfollows_df.to_csv(\"follows.csv\", header=False, index=False)\ncity_df.to_csv(\"city.csv\", header=False, index=False)\nlives_in_df.to_csv(\"lives_in.csv\", header=False, index=False)\n\n\n\nUsing Kùzu\nImport Kùzu.\n\nimport kuzu\n\nCreate an empty database and connect to it with Python API.\n\nimport shutil\nshutil.rmtree(\"./test\", ignore_errors=True)\n\n\ndb = kuzu.Database('./test', buffer_pool_size=1024**3)\nconn = kuzu.Connection(db)\n\nCreate schemas in Kùzu.\n\nconn.execute(\"CREATE NODE TABLE User(name STRING, age INT64, PRIMARY KEY (name))\")\nconn.execute(\"CREATE NODE TABLE City(name STRING, population INT64, PRIMARY KEY (name))\")\nconn.execute(\"CREATE REL TABLE Follows(FROM User TO User, since INT64)\")\nconn.execute(\"CREATE REL TABLE LivesIn(FROM User TO City)\")\n\n&lt;kuzu.query_result.QueryResult at 0x7abff7547be0&gt;\n\n\nLoad data from CSV files into Kùzu.\n\nconn.execute('COPY User FROM \"user.csv\";')\nconn.execute('COPY City FROM \"city.csv\";')\nconn.execute('COPY Follows FROM \"follows.csv\";')\nconn.execute('COPY LivesIn FROM \"lives_in.csv\";')\n\n&lt;kuzu.query_result.QueryResult at 0x7ac02a08a3e0&gt;\n\n\nExecute a simple query and iterate through the results.\n\nresults = conn.execute('MATCH (u:User) RETURN u.name, u.age;')\nwhile results.has_next():\n    print(results.get_next())\nresults.close()\n\n['Adam', 30]\n['Karissa', 40]\n['Zhang', 50]\n['Noura', 25]\n\n\nAlternatively, the Python API can also output results as a Pandas dataframe.\n\nresults = conn.execute('MATCH (a:User) - [f:Follows] -&gt; (b:User) RETURN a.name, f.since, b.name;')\n\n\nresults.get_as_df()\n\n\n  \n    \n\n\n\n\n\n\na.name\nf.since\nb.name\n\n\n\n\n0\nAdam\n2020\nKarissa\n\n\n1\nAdam\n2020\nZhang\n\n\n2\nKarissa\n2021\nZhang\n\n\n3\nZhang\n2022\nNoura\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThe Python API can also output results in Apache Arrow format.\n\nresults = conn.execute('MATCH (u:User) RETURN u.name, u.age;')\nresults.get_as_arrow(chunk_size=100)\n\npyarrow.Table\nu.name: string\nu.age: int64\n----\nu.name: [[\"Adam\",\"Karissa\",\"Zhang\",\"Noura\"]]\nu.age: [[30,40,50,25]]\n\n\n\n%%capture\n!pip install yfiles_jupyter_graphs\n\n\nfrom yfiles_jupyter_graphs import GraphWidget\nfrom google.colab import output\noutput.enable_custom_widget_manager()\nresult = conn.execute('MATCH (a:User) - [f:Follows] -&gt; (b:User) RETURN a,f,b')\nGraphWidget(graph = result.get_as_networkx())\n\n\n\n\n\ng = GraphWidget(graph = result.get_as_networkx())\ng.show()\n\n\n\n\n\ng.set_graph_layout('Hierarchic')\n\n\ng.show()\n\n\n\n\n\ng.node_label_mapping='name'\n\n\ng.show()\n\n\n\n\n\ng.edge_label_mapping = '_label'\n\n\ng.show()\n\n\n\n\n\ng.show_cypher(\"MATCH (a:User) - [f:Follows] -&gt; (b:User) RETURN a,f,b LIMIT 3\")\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-26-97da18912208&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 g.show_cypher(\"MATCH (a:User) - [f:Follows] -&gt; (b:User) RETURN a,f,b LIMIT 3\")\n\nAttributeError: 'GraphWidget' object has no attribute 'show_cypher'\n\n\n\n\ng.set_edge_color_mapping(lambda edge : \"blue\" if edge[\"properties\"][\"_label\"] == \"Follows\" else \"black\")\n\n\n\ng.show()\n\n\n\n\n\nstyles = {\n    \"User\": {\"color\":\"#6C7400\", \"shape\":\"ellipse\", \"label\":\"name\"}\n  }\n\n\ng.set_node_styles_mapping(lambda node : styles.get(node[\"properties\"][\"label\"], {}))\n\n\n\ng.show()\n\n\n\n\n\ng.set_node_label_mapping(lambda node : node[\"properties\"][styles.get(node[\"properties\"][\"label\"], {\"label\":\"label\"})[\"label\"]])\n\n\ng.show()\n\n\n\n\n\nprint(g.node_type_mapping.__doc__)\n\nThe default type mapping for nodes.\n\n        Provides the mapped node color to distinguish different node types\n\n        Parameters\n        ----------\n        index: int (optional)\n        node: typing.Dict\n\n        Notes\n        -----\n        This is the default value for the `node_type_mapping` property.\n        Can be 'overwritten' by setting the property\n        with a function of the same signature.\n\n        If the given mapping function has only one parameter (that is not typed as int),\n        then it will be called with the element (typing.Dict) as first parameter.\n\n        Example\n        -------\n        .. code::\n\n           from yfiles_jupyter_graphs import GraphWidget\n           w = GraphWidget()\n           def custom_node_type_mapping(node: typing.Dict):\n           ...\n           w.set_node_type_mapping(custom_node_type_mapping)\n\n        Returns\n        -------\n        type: None\n\n        \n\n\n\ng.get_node_type_mapping()\n\n\n    yfiles_jupyter_graphs.widget.GraphWidget.default_node_type_mappingdef default_node_type_mapping(index: int, node: TDict)/usr/local/lib/python3.10/dist-packages/yfiles_jupyter_graphs/widget.pyThe default type mapping for nodes.\n\nProvides the mapped node color to distinguish different node types\n\nParameters\n----------\nindex: int (optional)\nnode: typing.Dict\n\nNotes\n-----\nThis is the default value for the `node_type_mapping` property.\nCan be 'overwritten' by setting the property\nwith a function of the same signature.\n\nIf the given mapping function has only one parameter (that is not typed as int),\nthen it will be called with the element (typing.Dict) as first parameter.\n\nExample\n-------\n.. code::\n\n   from yfiles_jupyter_graphs import GraphWidget\n   w = GraphWidget()\n   def custom_node_type_mapping(node: typing.Dict):\n   ...\n   w.set_node_type_mapping(custom_node_type_mapping)\n\nReturns\n-------\ntype: None\n      \n      \n\n\nLet’s set the new type mapping.\n\nfrom typing import Dict\n\n\ndef custom_node_type_mapping(node: Dict):\n    \"\"\"assign type accordingly\"\"\"\n    return node['properties']['_id']['offset']\n\n\ng.set_node_type_mapping(custom_node_type_mapping)\ng.get_node_type_mapping()\n\n\n    custom_node_type_mappingdef custom_node_type_mapping(node: Dict)/content/&lt;ipython-input-42-c19df59731f0&gt;assign type accordingly\n\n\n\ndisplay(g)\n\n\n\n\n\ncolors = [\"#17bebb\", \"#ffc914\", \"#0b7189\", \"#ff6c00\", '#76b041']\ndef custom_node_color_mapping(node: Dict):\n    \"\"\"assign colors based on type\"\"\"\n    return colors[node['properties']['_id']['offset']]\ng.set_node_color_mapping(custom_node_color_mapping)\ng.get_node_type_mapping()\n\n\n    custom_node_type_mappingdef custom_node_type_mapping(node: Dict)/content/&lt;ipython-input-42-c19df59731f0&gt;assign type accordingly\n\n\n\ndisplay(g)\n\n\n\n\nIf a node type mapping is deleted, the layout mapping reverts back to the default mapping.\n\ng.del_node_color_mapping()\ng.get_node_color_mapping()\n\n\n    yfiles_jupyter_graphs.widget.GraphWidget.default_node_color_mappingdef default_node_color_mapping(index: int, node: TDict)/usr/local/lib/python3.10/dist-packages/yfiles_jupyter_graphs/widget.pyThe default color mapping for nodes.\n\nProvides constant value of '#15AFAC' for all nodes, or different colors per label/type when importing a Neo4j\ngraph.\n\nParameters\n----------\nindex: int (optional)\nnode: typing.Dict\n\nNotes\n-----\nThis is the default value for the `node_color_mapping` property.\nCan be 'overwritten' by setting the property\nwith a function of the same signature.\n\nIf the given mapping function has only one parameter (that is not typed as int),\nthen it will be called with the element (typing.Dict) as first parameter.\n\nExample\n-------\n.. code::\n\n   from yfiles_jupyter_graphs import GraphWidget\n   w = GraphWidget()\n   def custom_node_color_mapping(node: typing.Dict):\n   ...\n   w.set_node_color_mapping(custom_node_color_mapping)\n\nReturns\n-------\ncolor: str\n    css color value\n\nReferences\n----------\ncss color value &lt;https://developer.mozilla.org/en-US/docs/Web/CSS/color_value&gt;\n\nyFiles docs Fill api &lt;https://docs.yworks.com/yfileshtml/#/api/Fill&gt;\n      \n      \n\n\n\ng.show()",
    "crumbs": [
      "General Kuzu Demo"
    ]
  },
  {
    "objectID": "docs/demo/old-kuzu-use.html",
    "href": "docs/demo/old-kuzu-use.html",
    "title": "Quarto graph experiment - leveraging Kuzu, PyVis",
    "section": "",
    "text": "import micropip,networkx as nx,pandas as pd,numpy as np,matplotlib.pyplot as plt,pyodide_js,js\nfrom IPython.display import display, HTML\nawait pyodide_js.loadPackage(\"https://storage.googleapis.com/ibis-wasm-wheels/pyarrow-16.0.0.dev2661%2Bg9bddb87fd-cp311-cp311-emscripten_3_1_46_wasm32.whl\")\nawait micropip.install([\"tzdata\",\"pyvis\",\"kuzu_wasm\"])\nimport pyarrow as pa,kuzu_wasm\nfrom kuzu_wasm.utils import *\n\n\nkuzu = await kuzu_wasm.init(\"https://kuzu-lab.netlify.app/package/dist/kuzu.js\")\ndb = await kuzu.Database()\nconn = await kuzu.Connection(db)\n\n\n# get remote csv to wasm filesystem\nkuzu.FS.writeFile(\"/follows.csv\",await (await js.fetch(\"https://raw.githubusercontent.com/kuzudb/kuzu/master/dataset/demo-db/csv/follows.csv\")).text())\nkuzu.FS.writeFile(\"/city.csv\",await (await js.fetch(\"https://raw.githubusercontent.com/kuzudb/kuzu/master/dataset/demo-db/csv/city.csv\")).text())\nkuzu.FS.writeFile(\"/lives-in.csv\",await (await js.fetch(\"https://raw.githubusercontent.com/kuzudb/kuzu/master/dataset/demo-db/csv/lives-in.csv\")).text())\nkuzu.FS.writeFile(\"/user.csv\",await (await js.fetch(\"https://raw.githubusercontent.com/kuzudb/kuzu/master/dataset/demo-db/csv/user.csv\")).text())\n\n\n\n# create schema and import data\nawait conn.execute(\"CREATE NODE TABLE User(name STRING, age INT64, PRIMARY KEY (name))\")\nawait conn.execute(\"CREATE NODE TABLE City(name STRING, population INT64, PRIMARY KEY (name))\")\nawait conn.execute(\"CREATE REL TABLE Follows(FROM User TO User, since INT64)\")\nawait conn.execute(\"CREATE REL TABLE LivesIn(FROM User TO City)\")\nawait conn.execute('COPY User FROM \"/user.csv\";')\nawait conn.execute('COPY City FROM \"/city.csv\";')\nawait conn.execute('COPY Follows FROM \"/follows.csv\";')\nawait conn.execute('COPY LivesIn FROM \"/lives_in.csv\";')\n\n\nres = await conn.execute(\"MATCH (a:User)-[f:Follows]-&gt;(b:User)RETURN a.name, b.name, f.since\")\ndf = toDf(res)\ndf\n\n\nfrom IPython.display import HTML\nfrom pyvis.network import Network\nres_2 = await conn.execute(\"MATCH (a:User)-[f:Follows]-&gt;(b:User)RETURN a,f,b\")\nG = toNetworkx(res_2,directed = True)\ng = Network(notebook=True, cdn_resources='remote',directed = True,neighborhood_highlight = True)\ng.from_nx(G)\nfor i in range(len(g.nodes)):\n    g.nodes[i][\"label\"] = g.nodes[i][\"name\"]\nhtml = g.generate_html(notebook=True)\nHTML(f'&lt;div style=\"height:{g.height}\"&gt;{html}&lt;/div&gt;', metadata={\"isolated\": True}, )",
    "crumbs": [
      "Quarto graph experiment - leveraging Kuzu, PyVis"
    ]
  },
  {
    "objectID": "docs/notebooks/Export_Query_Results_to_NetworkX.html",
    "href": "docs/notebooks/Export_Query_Results_to_NetworkX.html",
    "title": "Kuzu - Export Query Results to NetworkX",
    "section": "",
    "text": "One of the overarching goals of Kùzu is to function as the go-to graph database for data science use cases. NetworkX is a popular library in Python for graph algorithms and data science. In this notebook, we demonstrate Kùzu’s ease of use in exporting subgraphs to the NetworkX format using the get_as_networkx() function. In addition, the following two capabilities are demonstrated.\n\nGraph Visualization: In the first part, we simply draw the nodes and edges in the results using NetworkX.\nPageRank: In the second part, we compute PageRank on an extracted subgraph of nodes and edges; store these values back in Kùzu’s node tables and query them.\n\n\n\nWe will be working on the popular MovieLens dataset from GroupLens. The schema of the dataset is illustrated as below:\n\nWe use the small version of the dataset, which contains 610 user nodes, 9724 movie nodes, 100863 rates edges, and 3684 tags edges.\nNecessary Package Installations and Imports\n\n!pip install kuzu scipy networkx pandas\n!pip install matplotlib ipywidgets yfiles-jupyter-graphs\n\nRequirement already satisfied: kuzu in /usr/local/lib/python3.10/dist-packages (0.3.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\nRequirement already satisfied: numpy&lt;1.28.0,&gt;=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas) (1.16.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\nRequirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\nRequirement already satisfied: yfiles-jupyter-graphs in /usr/local/lib/python3.10/dist-packages (1.6.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy&gt;=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: ipykernel&gt;=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\nRequirement already satisfied: traitlets&gt;=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.6)\nRequirement already satisfied: ipython&gt;=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\nRequirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.10)\nRequirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets) (6.1.12)\nRequirement already satisfied: tornado&gt;=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets) (6.3.3)\nRequirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (67.7.2)\nRequirement already satisfied: jedi&gt;=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.19.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (3.0.43)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (2.16.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.1.6)\nRequirement already satisfied: pexpect&gt;4.3 in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (4.9.0)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)\nRequirement already satisfied: notebook&gt;=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0-&gt;ipywidgets) (6.5.5)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi&gt;=0.16-&gt;ipython&gt;=4.0.0-&gt;ipywidgets) (0.8.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (3.1.3)\nRequirement already satisfied: pyzmq&lt;25,&gt;=17 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (23.2.1)\nRequirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (23.1.0)\nRequirement already satisfied: jupyter-core&gt;=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (5.7.1)\nRequirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (5.9.2)\nRequirement already satisfied: nbconvert&gt;=5 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (6.5.4)\nRequirement already satisfied: nest-asyncio&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.6.0)\nRequirement already satisfied: Send2Trash&gt;=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.8.2)\nRequirement already satisfied: terminado&gt;=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.18.0)\nRequirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.20.0)\nRequirement already satisfied: nbclassic&gt;=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.0.0)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect&gt;4.3-&gt;ipython&gt;=4.0.0-&gt;ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython&gt;=4.0.0-&gt;ipywidgets) (0.2.13)\nRequirement already satisfied: platformdirs&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core&gt;=4.6.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (4.2.0)\nRequirement already satisfied: jupyter-server&gt;=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.24.0)\nRequirement already satisfied: notebook-shim&gt;=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.2.4)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (4.9.4)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (4.12.3)\nRequirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (6.1.0)\nRequirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.7.1)\nRequirement already satisfied: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.4)\nRequirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.3.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (2.1.5)\nRequirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.8.4)\nRequirement already satisfied: nbclient&gt;=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.9.0)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.5.1)\nRequirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.2.1)\nRequirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (2.19.1)\nRequirement already satisfied: jsonschema&gt;=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (4.19.2)\nRequirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (21.2.0)\nRequirement already satisfied: attrs&gt;=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (23.2.0)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (2023.12.1)\nRequirement already satisfied: referencing&gt;=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.33.0)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.18.0)\nRequirement already satisfied: anyio&lt;4,&gt;=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server&gt;=1.8-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (3.7.1)\nRequirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server&gt;=1.8-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.7.0)\nRequirement already satisfied: cffi&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings-&gt;argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.16.0)\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4-&gt;nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (2.5)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach-&gt;nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.5.1)\nRequirement already satisfied: idna&gt;=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server&gt;=1.8-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (3.6)\nRequirement already satisfied: sniffio&gt;=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server&gt;=1.8-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server&gt;=1.8-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi&gt;=1.0.1-&gt;argon2-cffi-bindings-&gt;argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (2.21)\n\n\n\nimport kuzu\nimport pandas as pd\nimport networkx as nx\n\nWget Dataset Files and Import Into Kùzu\n\n!rm -rf *.csv ml-small_db\n\n\n!wget https://kuzudb.com/data/movie-lens/movies.csv\n!wget https://kuzudb.com/data/movie-lens/users.csv\n!wget https://kuzudb.com/data/movie-lens/ratings.csv\n!wget https://kuzudb.com/data/movie-lens/tags.csv\n\n--2024-03-08 20:27:20--  https://kuzudb.com/data/movie-lens/movies.csv\nResolving kuzudb.com (kuzudb.com)... 188.114.97.0, 188.114.96.0, 2a06:98c1:3120::, ...\nConnecting to kuzudb.com (kuzudb.com)|188.114.97.0|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/csv]\nSaving to: ‘movies.csv’\n\nmovies.csv              [ &lt;=&gt;                ] 520.78K  --.-KB/s    in 0.01s   \n\n2024-03-08 20:27:21 (36.4 MB/s) - ‘movies.csv’ saved [533280]\n\n--2024-03-08 20:27:21--  https://kuzudb.com/data/movie-lens/users.csv\nResolving kuzudb.com (kuzudb.com)... 188.114.97.0, 188.114.96.0, 2a06:98c1:3120::, ...\nConnecting to kuzudb.com (kuzudb.com)|188.114.97.0|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/csv]\nSaving to: ‘users.csv’\n\nusers.csv               [ &lt;=&gt;                ]   2.28K  --.-KB/s    in 0s      \n\n2024-03-08 20:27:21 (37.6 MB/s) - ‘users.csv’ saved [2338]\n\n--2024-03-08 20:27:21--  https://kuzudb.com/data/movie-lens/ratings.csv\nResolving kuzudb.com (kuzudb.com)... 188.114.97.0, 188.114.96.0, 2a06:98c1:3120::, ...\nConnecting to kuzudb.com (kuzudb.com)|188.114.97.0|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/csv]\nSaving to: ‘ratings.csv’\n\nratings.csv             [ &lt;=&gt;                ]   2.27M  --.-KB/s    in 0.03s   \n\n2024-03-08 20:27:21 (79.0 MB/s) - ‘ratings.csv’ saved [2382885]\n\n--2024-03-08 20:27:21--  https://kuzudb.com/data/movie-lens/tags.csv\nResolving kuzudb.com (kuzudb.com)... 188.114.97.0, 188.114.96.0, 2a06:98c1:3120::, ...\nConnecting to kuzudb.com (kuzudb.com)|188.114.97.0|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/csv]\nSaving to: ‘tags.csv’\n\ntags.csv                [ &lt;=&gt;                ] 112.28K  --.-KB/s    in 0.01s   \n\n2024-03-08 20:27:21 (11.5 MB/s) - ‘tags.csv’ saved [114977]\n\n\n\nCreate schemas in Kùzu and import the unzipped csv files into Kùzu using COPY FROM clause.\n\nimport shutil\n\ndb_path = './ml-small_db'\nshutil.rmtree(db_path, ignore_errors=True)\n\ndef load_data(connection):\n    connection.execute('CREATE NODE TABLE Movie (movieId INT64, year INT64, title STRING, genres STRING, PRIMARY KEY (movieId))')\n    connection.execute('CREATE NODE TABLE User (userId INT64, PRIMARY KEY (userId))')\n    connection.execute('CREATE REL TABLE Rating (FROM User TO Movie, rating DOUBLE, timestamp INT64)')\n    connection.execute('CREATE REL TABLE Tags (FROM User TO Movie, tag STRING, timestamp INT64)')\n\n    connection.execute('COPY Movie FROM \"./movies.csv\" (HEADER=TRUE)')\n    connection.execute('COPY User FROM \"./users.csv\" (HEADER=TRUE)')\n    connection.execute('COPY Rating FROM \"./ratings.csv\" (HEADER=TRUE)')\n    connection.execute('COPY Tags FROM \"./tags.csv\" (HEADER=TRUE)')\n\ndb = kuzu.Database(db_path)\nconn = kuzu.Connection(db)\nload_data(conn)\n\n\n\nExtract a subgraph of 250 ratings edges using Cypher; convert to a NetworkX graph G; and draw a node-link visualization.\n\nres = conn.execute('MATCH (u:User)-[r:Rating]-&gt;(m:Movie) RETURN u, r, m LIMIT 250')\nG = res.get_as_networkx(directed=False)\ncolors = ['red' if G.nodes[node]['_label'] == 'User' else 'blue' for node in list(G.nodes())]\nnx.draw_spring(G, node_color=colors, node_size=40)\n\n\n\n\n\n\n\n\n\n\n\nyfiles-jupyter-graphs is a free tool to generate interactive visualizations of graphs within a Jupyter notebook environment. Follow the docs to install the dependencies.\nThe following visualization uses the same graph G from above that consists of 250 movie ratings edges.\n\ntry:\n  import google.colab\n  from google.colab import output\n  output.enable_custom_widget_manager()\nexcept:\n  pass\n\n\nfrom typing import Union, Any\n\ndef custom_node_color_mapping(node: dict[str, Any]):\n    \"\"\"let the color be orange or blue if the index is even or odd respectively\"\"\"\n    return (\"#eb4934\" if node['properties']['_label'] == \"User\" else \"#2456d4\")\n\n\nfrom yfiles_jupyter_graphs import GraphWidget\n\nw = GraphWidget(graph=G)\nw.set_sidebar(enabled=False)\nw.set_node_color_mapping(custom_node_color_mapping)\ndisplay(w)\n\n\n\n\n\n\n\nWe extract only the subgraph between users and movies (so ignoring tags) and convert to a NetworkX graph G.\n\nres = conn.execute('MATCH (u:User)-[r:Rating]-&gt;(m:Movie) RETURN u, r, m')\nG = res.get_as_networkx(directed=False)\n\nNext compute PageRanks of users and movies.\n\npageranks = nx.pagerank(G)\n\nPut returned pageranks into a page_rank_df dataframe and get a list of movies and their pageranks into a movie_df data frame.\n\npagerank_df = pd.DataFrame.from_dict(pageranks, orient=\"index\", columns=[\"pagerank\"])\nmovie_df = pagerank_df[pagerank_df.index.str.contains(\"Movie\")]\nmovie_df.index = movie_df.index.str.replace(\"Movie_\", \"\").astype(int)\nmovie_df = movie_df.reset_index(names=[\"id\"])\nprint(f\"Calculated pageranks for {len(movie_df)} nodes\")\nmovie_df.sort_values(by=\"pagerank\", ascending=False).head()\n\nCalculated pageranks for 9724 nodes\n\n\n\n  \n    \n\n\n\n\n\n\nid\npagerank\n\n\n\n\n20\n356\n0.001155\n\n\n232\n318\n0.001099\n\n\n16\n296\n0.001075\n\n\n166\n2571\n0.001006\n\n\n34\n593\n0.000987\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nuser_df = pagerank_df[pagerank_df.index.str.contains(\"User\")]\nuser_df.index = user_df.index.str.replace(\"User_\", \"\").astype(int)\nuser_df = user_df.reset_index(names=[\"id\"])\nuser_df.sort_values(by=\"pagerank\", ascending=False).head()\n\n\n  \n    \n\n\n\n\n\n\nid\npagerank\n\n\n\n\n598\n599\n0.016401\n\n\n413\n414\n0.014711\n\n\n473\n474\n0.014380\n\n\n447\n448\n0.012942\n\n\n609\n610\n0.008492\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\n\n\nAlter the movie and user table schemas by adding a new pagerank property to them (of type float64).\n\ntry:\n  # Alter original node table schemas to add pageranks\n  conn.execute(\"ALTER TABLE Movie ADD pagerank DOUBLE DEFAULT 0.0;\")\n  conn.execute(\"ALTER TABLE User ADD pagerank DOUBLE DEFAULT 0.0;\")\nexcept RuntimeError:\n  # If the column already exists, do nothing\n  pass\n\n\n\n\nThe next feature demonstrated is a powerful one: Kùzu can natively scan Pandas DataFrames in a zero-copy manner, by using the LOAD FROM clause on the variable name that stores the DataFrame. Once the scan is done, the values are accessible by Kùzu via Cypher. This approach is used to read the computed pageranks (which are in a Pandas DataFrame) to the respective node tables in Kùzu.\n\n# Copy pagerank values to movie nodes\nx = conn.execute(\n  \"\"\"\n  LOAD FROM movie_df\n  MERGE (m:Movie {movieId: id})\n  ON MATCH SET m.pagerank = pagerank\n  RETURN m.movieId AS movieId, m.pagerank AS pagerank;\n  \"\"\"\n)\nx.get_as_df().head()\n\n\n  \n    \n\n\n\n\n\n\nmovieId\npagerank\n\n\n\n\n0\n1\n0.000776\n\n\n1\n3\n0.000200\n\n\n2\n6\n0.000368\n\n\n3\n47\n0.000707\n\n\n4\n50\n0.000724\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Copy user pagerank values to user nodes\ny = conn.execute(\n  \"\"\"\n  LOAD FROM user_df\n  MERGE (u:User {userId: id})\n  ON MATCH SET u.pagerank = pagerank\n  RETURN u.userId As userId, u.pagerank AS pagerank;\n  \"\"\"\n)\ny.get_as_df().head()\n\n\n  \n    \n\n\n\n\n\n\nuserId\npagerank\n\n\n\n\n0\n1\n0.000867\n\n\n1\n2\n0.000134\n\n\n2\n3\n0.000254\n\n\n3\n4\n0.000929\n\n\n4\n5\n0.000151\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe next find the top 20 pagerank movies and then users.\n\nconn.execute('MATCH (m:Movie) RETURN m.title, m.pagerank ORDER BY m.pagerank DESC LIMIT 10').get_as_df()\n\n\n  \n    \n\n\n\n\n\n\nm.title\nm.pagerank\n\n\n\n\n0\nForrest Gump (1994)\n0.001155\n\n\n1\nShawshank Redemption, The (1994)\n0.001099\n\n\n2\nPulp Fiction (1994)\n0.001075\n\n\n3\nMatrix, The (1999)\n0.001006\n\n\n4\nSilence of the Lambs, The (1991)\n0.000987\n\n\n5\nStar Wars: Episode IV - A New Hope (1977)\n0.000903\n\n\n6\nJurassic Park (1993)\n0.000825\n\n\n7\nBraveheart (1995)\n0.000810\n\n\n8\nFight Club (1999)\n0.000797\n\n\n9\nSchindler's List (1993)\n0.000778\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nconn.execute('MATCH (u:User) RETURN u.userId, u.pagerank ORDER BY u.pagerank DESC LIMIT 10').get_as_df()\n\n\n  \n    \n\n\n\n\n\n\nu.userId\nu.pagerank\n\n\n\n\n0\n599\n0.016401\n\n\n1\n414\n0.014711\n\n\n2\n474\n0.014380\n\n\n3\n448\n0.012942\n\n\n4\n610\n0.008492\n\n\n5\n606\n0.007245\n\n\n6\n274\n0.006253\n\n\n7\n89\n0.006083\n\n\n8\n380\n0.005997\n\n\n9\n318\n0.005920\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nAs a final example, we find the average ratings from a highly influential (i.e, high pagerank score) user and highly influential movies. To filter high pagerank users, we first set a threshold of a user’s score being above or below the top 10 pagerank scores for all users. Any user with a pagerank score above this value is considered an “influential” user. We repeat this same computation to find the threshold for movies. Then we find all ratings between influential users and movies and take their average.\n\nuser_pr_threshold = conn.execute(\n  \"\"\"\n  MATCH (u:User)\n  WITH u.pagerank AS user_pr_threshold\n  ORDER BY u.pagerank DESC LIMIT 10\n  RETURN min(user_pr_threshold)\n  \"\"\"\n).get_as_df().iloc[0,0];\nuser_pr_threshold\n\n0.005919808556361484\n\n\n\nmovie_pr_threshold = conn.execute(\n    \"\"\"\n    MATCH (m:Movie)\n    WITH m.pagerank AS movie_pr_threshold\n    ORDER BY m.pagerank DESC LIMIT 10\n    RETURN min(movie_pr_threshold)\n    \"\"\"\n).get_as_df().iloc[0,0];\nmovie_pr_threshold\n\n0.0007781856282699511\n\n\n\navg_rating_df = conn.execute(\n    \"\"\"\n    MATCH (u:User)-[r:Rating]-&gt;(m:Movie)\n    WHERE u.pagerank &gt; $user_pr_threshold  AND m.pagerank &gt; $movie_pr_threshold\n    RETURN avg(r.rating) as avgRBtwHighPRUserMovies;\n    \"\"\",\n    parameters={\"user_pr_threshold\": user_pr_threshold, \"movie_pr_threshold\": movie_pr_threshold}\n).get_as_df()\navg_rating_df.head()\n\n\n  \n    \n\n\n\n\n\n\navgRBtwHighPRUserMovies\n\n\n\n\n0\n4.239437",
    "crumbs": [
      "Kuzu - Export Query Results to NetworkX"
    ]
  },
  {
    "objectID": "docs/notebooks/Export_Query_Results_to_NetworkX.html#movielens-dataset",
    "href": "docs/notebooks/Export_Query_Results_to_NetworkX.html#movielens-dataset",
    "title": "Kuzu - Export Query Results to NetworkX",
    "section": "",
    "text": "We will be working on the popular MovieLens dataset from GroupLens. The schema of the dataset is illustrated as below:\n\nWe use the small version of the dataset, which contains 610 user nodes, 9724 movie nodes, 100863 rates edges, and 3684 tags edges.\nNecessary Package Installations and Imports\n\n!pip install kuzu scipy networkx pandas\n!pip install matplotlib ipywidgets yfiles-jupyter-graphs\n\nRequirement already satisfied: kuzu in /usr/local/lib/python3.10/dist-packages (0.3.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\nRequirement already satisfied: numpy&lt;1.28.0,&gt;=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas) (1.16.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\nRequirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\nRequirement already satisfied: yfiles-jupyter-graphs in /usr/local/lib/python3.10/dist-packages (1.6.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy&gt;=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: ipykernel&gt;=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\nRequirement already satisfied: traitlets&gt;=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.6)\nRequirement already satisfied: ipython&gt;=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\nRequirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.10)\nRequirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets) (6.1.12)\nRequirement already satisfied: tornado&gt;=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets) (6.3.3)\nRequirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (67.7.2)\nRequirement already satisfied: jedi&gt;=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.19.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (3.0.43)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (2.16.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (0.1.6)\nRequirement already satisfied: pexpect&gt;4.3 in /usr/local/lib/python3.10/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets) (4.9.0)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)\nRequirement already satisfied: notebook&gt;=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0-&gt;ipywidgets) (6.5.5)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi&gt;=0.16-&gt;ipython&gt;=4.0.0-&gt;ipywidgets) (0.8.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (3.1.3)\nRequirement already satisfied: pyzmq&lt;25,&gt;=17 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (23.2.1)\nRequirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (23.1.0)\nRequirement already satisfied: jupyter-core&gt;=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (5.7.1)\nRequirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (5.9.2)\nRequirement already satisfied: nbconvert&gt;=5 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (6.5.4)\nRequirement already satisfied: nest-asyncio&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.6.0)\nRequirement already satisfied: Send2Trash&gt;=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.8.2)\nRequirement already satisfied: terminado&gt;=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.18.0)\nRequirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.20.0)\nRequirement already satisfied: nbclassic&gt;=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.0.0)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect&gt;4.3-&gt;ipython&gt;=4.0.0-&gt;ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython&gt;=4.0.0-&gt;ipywidgets) (0.2.13)\nRequirement already satisfied: platformdirs&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core&gt;=4.6.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (4.2.0)\nRequirement already satisfied: jupyter-server&gt;=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.24.0)\nRequirement already satisfied: notebook-shim&gt;=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.2.4)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (4.9.4)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (4.12.3)\nRequirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (6.1.0)\nRequirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.7.1)\nRequirement already satisfied: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.4)\nRequirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.3.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (2.1.5)\nRequirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.8.4)\nRequirement already satisfied: nbclient&gt;=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.9.0)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.5.1)\nRequirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.2.1)\nRequirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (2.19.1)\nRequirement already satisfied: jsonschema&gt;=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (4.19.2)\nRequirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (21.2.0)\nRequirement already satisfied: attrs&gt;=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (23.2.0)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (2023.12.1)\nRequirement already satisfied: referencing&gt;=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.33.0)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.18.0)\nRequirement already satisfied: anyio&lt;4,&gt;=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server&gt;=1.8-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (3.7.1)\nRequirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server&gt;=1.8-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.7.0)\nRequirement already satisfied: cffi&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings-&gt;argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.16.0)\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4-&gt;nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (2.5)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach-&gt;nbconvert&gt;=5-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (0.5.1)\nRequirement already satisfied: idna&gt;=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server&gt;=1.8-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (3.6)\nRequirement already satisfied: sniffio&gt;=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server&gt;=1.8-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio&lt;4,&gt;=3.1.0-&gt;jupyter-server&gt;=1.8-&gt;nbclassic&gt;=0.4.7-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (1.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi&gt;=1.0.1-&gt;argon2-cffi-bindings-&gt;argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets) (2.21)\n\n\n\nimport kuzu\nimport pandas as pd\nimport networkx as nx\n\nWget Dataset Files and Import Into Kùzu\n\n!rm -rf *.csv ml-small_db\n\n\n!wget https://kuzudb.com/data/movie-lens/movies.csv\n!wget https://kuzudb.com/data/movie-lens/users.csv\n!wget https://kuzudb.com/data/movie-lens/ratings.csv\n!wget https://kuzudb.com/data/movie-lens/tags.csv\n\n--2024-03-08 20:27:20--  https://kuzudb.com/data/movie-lens/movies.csv\nResolving kuzudb.com (kuzudb.com)... 188.114.97.0, 188.114.96.0, 2a06:98c1:3120::, ...\nConnecting to kuzudb.com (kuzudb.com)|188.114.97.0|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/csv]\nSaving to: ‘movies.csv’\n\nmovies.csv              [ &lt;=&gt;                ] 520.78K  --.-KB/s    in 0.01s   \n\n2024-03-08 20:27:21 (36.4 MB/s) - ‘movies.csv’ saved [533280]\n\n--2024-03-08 20:27:21--  https://kuzudb.com/data/movie-lens/users.csv\nResolving kuzudb.com (kuzudb.com)... 188.114.97.0, 188.114.96.0, 2a06:98c1:3120::, ...\nConnecting to kuzudb.com (kuzudb.com)|188.114.97.0|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/csv]\nSaving to: ‘users.csv’\n\nusers.csv               [ &lt;=&gt;                ]   2.28K  --.-KB/s    in 0s      \n\n2024-03-08 20:27:21 (37.6 MB/s) - ‘users.csv’ saved [2338]\n\n--2024-03-08 20:27:21--  https://kuzudb.com/data/movie-lens/ratings.csv\nResolving kuzudb.com (kuzudb.com)... 188.114.97.0, 188.114.96.0, 2a06:98c1:3120::, ...\nConnecting to kuzudb.com (kuzudb.com)|188.114.97.0|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/csv]\nSaving to: ‘ratings.csv’\n\nratings.csv             [ &lt;=&gt;                ]   2.27M  --.-KB/s    in 0.03s   \n\n2024-03-08 20:27:21 (79.0 MB/s) - ‘ratings.csv’ saved [2382885]\n\n--2024-03-08 20:27:21--  https://kuzudb.com/data/movie-lens/tags.csv\nResolving kuzudb.com (kuzudb.com)... 188.114.97.0, 188.114.96.0, 2a06:98c1:3120::, ...\nConnecting to kuzudb.com (kuzudb.com)|188.114.97.0|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/csv]\nSaving to: ‘tags.csv’\n\ntags.csv                [ &lt;=&gt;                ] 112.28K  --.-KB/s    in 0.01s   \n\n2024-03-08 20:27:21 (11.5 MB/s) - ‘tags.csv’ saved [114977]\n\n\n\nCreate schemas in Kùzu and import the unzipped csv files into Kùzu using COPY FROM clause.\n\nimport shutil\n\ndb_path = './ml-small_db'\nshutil.rmtree(db_path, ignore_errors=True)\n\ndef load_data(connection):\n    connection.execute('CREATE NODE TABLE Movie (movieId INT64, year INT64, title STRING, genres STRING, PRIMARY KEY (movieId))')\n    connection.execute('CREATE NODE TABLE User (userId INT64, PRIMARY KEY (userId))')\n    connection.execute('CREATE REL TABLE Rating (FROM User TO Movie, rating DOUBLE, timestamp INT64)')\n    connection.execute('CREATE REL TABLE Tags (FROM User TO Movie, tag STRING, timestamp INT64)')\n\n    connection.execute('COPY Movie FROM \"./movies.csv\" (HEADER=TRUE)')\n    connection.execute('COPY User FROM \"./users.csv\" (HEADER=TRUE)')\n    connection.execute('COPY Rating FROM \"./ratings.csv\" (HEADER=TRUE)')\n    connection.execute('COPY Tags FROM \"./tags.csv\" (HEADER=TRUE)')\n\ndb = kuzu.Database(db_path)\nconn = kuzu.Connection(db)\nload_data(conn)\n\n\n\nExtract a subgraph of 250 ratings edges using Cypher; convert to a NetworkX graph G; and draw a node-link visualization.\n\nres = conn.execute('MATCH (u:User)-[r:Rating]-&gt;(m:Movie) RETURN u, r, m LIMIT 250')\nG = res.get_as_networkx(directed=False)\ncolors = ['red' if G.nodes[node]['_label'] == 'User' else 'blue' for node in list(G.nodes())]\nnx.draw_spring(G, node_color=colors, node_size=40)\n\n\n\n\n\n\n\n\n\n\n\nyfiles-jupyter-graphs is a free tool to generate interactive visualizations of graphs within a Jupyter notebook environment. Follow the docs to install the dependencies.\nThe following visualization uses the same graph G from above that consists of 250 movie ratings edges.\n\ntry:\n  import google.colab\n  from google.colab import output\n  output.enable_custom_widget_manager()\nexcept:\n  pass\n\n\nfrom typing import Union, Any\n\ndef custom_node_color_mapping(node: dict[str, Any]):\n    \"\"\"let the color be orange or blue if the index is even or odd respectively\"\"\"\n    return (\"#eb4934\" if node['properties']['_label'] == \"User\" else \"#2456d4\")\n\n\nfrom yfiles_jupyter_graphs import GraphWidget\n\nw = GraphWidget(graph=G)\nw.set_sidebar(enabled=False)\nw.set_node_color_mapping(custom_node_color_mapping)\ndisplay(w)\n\n\n\n\n\n\n\nWe extract only the subgraph between users and movies (so ignoring tags) and convert to a NetworkX graph G.\n\nres = conn.execute('MATCH (u:User)-[r:Rating]-&gt;(m:Movie) RETURN u, r, m')\nG = res.get_as_networkx(directed=False)\n\nNext compute PageRanks of users and movies.\n\npageranks = nx.pagerank(G)\n\nPut returned pageranks into a page_rank_df dataframe and get a list of movies and their pageranks into a movie_df data frame.\n\npagerank_df = pd.DataFrame.from_dict(pageranks, orient=\"index\", columns=[\"pagerank\"])\nmovie_df = pagerank_df[pagerank_df.index.str.contains(\"Movie\")]\nmovie_df.index = movie_df.index.str.replace(\"Movie_\", \"\").astype(int)\nmovie_df = movie_df.reset_index(names=[\"id\"])\nprint(f\"Calculated pageranks for {len(movie_df)} nodes\")\nmovie_df.sort_values(by=\"pagerank\", ascending=False).head()\n\nCalculated pageranks for 9724 nodes\n\n\n\n  \n    \n\n\n\n\n\n\nid\npagerank\n\n\n\n\n20\n356\n0.001155\n\n\n232\n318\n0.001099\n\n\n16\n296\n0.001075\n\n\n166\n2571\n0.001006\n\n\n34\n593\n0.000987\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nuser_df = pagerank_df[pagerank_df.index.str.contains(\"User\")]\nuser_df.index = user_df.index.str.replace(\"User_\", \"\").astype(int)\nuser_df = user_df.reset_index(names=[\"id\"])\nuser_df.sort_values(by=\"pagerank\", ascending=False).head()\n\n\n  \n    \n\n\n\n\n\n\nid\npagerank\n\n\n\n\n598\n599\n0.016401\n\n\n413\n414\n0.014711\n\n\n473\n474\n0.014380\n\n\n447\n448\n0.012942\n\n\n609\n610\n0.008492",
    "crumbs": [
      "Kuzu - Export Query Results to NetworkX"
    ]
  },
  {
    "objectID": "docs/notebooks/Export_Query_Results_to_NetworkX.html#update-node-schemas-with-pagerank-property",
    "href": "docs/notebooks/Export_Query_Results_to_NetworkX.html#update-node-schemas-with-pagerank-property",
    "title": "Kuzu - Export Query Results to NetworkX",
    "section": "",
    "text": "Alter the movie and user table schemas by adding a new pagerank property to them (of type float64).\n\ntry:\n  # Alter original node table schemas to add pageranks\n  conn.execute(\"ALTER TABLE Movie ADD pagerank DOUBLE DEFAULT 0.0;\")\n  conn.execute(\"ALTER TABLE User ADD pagerank DOUBLE DEFAULT 0.0;\")\nexcept RuntimeError:\n  # If the column already exists, do nothing\n  pass",
    "crumbs": [
      "Kuzu - Export Query Results to NetworkX"
    ]
  },
  {
    "objectID": "docs/notebooks/Export_Query_Results_to_NetworkX.html#scan-pandas-dataframe-and-copy-values-to-kùzu",
    "href": "docs/notebooks/Export_Query_Results_to_NetworkX.html#scan-pandas-dataframe-and-copy-values-to-kùzu",
    "title": "Kuzu - Export Query Results to NetworkX",
    "section": "",
    "text": "The next feature demonstrated is a powerful one: Kùzu can natively scan Pandas DataFrames in a zero-copy manner, by using the LOAD FROM clause on the variable name that stores the DataFrame. Once the scan is done, the values are accessible by Kùzu via Cypher. This approach is used to read the computed pageranks (which are in a Pandas DataFrame) to the respective node tables in Kùzu.\n\n# Copy pagerank values to movie nodes\nx = conn.execute(\n  \"\"\"\n  LOAD FROM movie_df\n  MERGE (m:Movie {movieId: id})\n  ON MATCH SET m.pagerank = pagerank\n  RETURN m.movieId AS movieId, m.pagerank AS pagerank;\n  \"\"\"\n)\nx.get_as_df().head()\n\n\n  \n    \n\n\n\n\n\n\nmovieId\npagerank\n\n\n\n\n0\n1\n0.000776\n\n\n1\n3\n0.000200\n\n\n2\n6\n0.000368\n\n\n3\n47\n0.000707\n\n\n4\n50\n0.000724\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Copy user pagerank values to user nodes\ny = conn.execute(\n  \"\"\"\n  LOAD FROM user_df\n  MERGE (u:User {userId: id})\n  ON MATCH SET u.pagerank = pagerank\n  RETURN u.userId As userId, u.pagerank AS pagerank;\n  \"\"\"\n)\ny.get_as_df().head()\n\n\n  \n    \n\n\n\n\n\n\nuserId\npagerank\n\n\n\n\n0\n1\n0.000867\n\n\n1\n2\n0.000134\n\n\n2\n3\n0.000254\n\n\n3\n4\n0.000929\n\n\n4\n5\n0.000151\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe next find the top 20 pagerank movies and then users.\n\nconn.execute('MATCH (m:Movie) RETURN m.title, m.pagerank ORDER BY m.pagerank DESC LIMIT 10').get_as_df()\n\n\n  \n    \n\n\n\n\n\n\nm.title\nm.pagerank\n\n\n\n\n0\nForrest Gump (1994)\n0.001155\n\n\n1\nShawshank Redemption, The (1994)\n0.001099\n\n\n2\nPulp Fiction (1994)\n0.001075\n\n\n3\nMatrix, The (1999)\n0.001006\n\n\n4\nSilence of the Lambs, The (1991)\n0.000987\n\n\n5\nStar Wars: Episode IV - A New Hope (1977)\n0.000903\n\n\n6\nJurassic Park (1993)\n0.000825\n\n\n7\nBraveheart (1995)\n0.000810\n\n\n8\nFight Club (1999)\n0.000797\n\n\n9\nSchindler's List (1993)\n0.000778\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nconn.execute('MATCH (u:User) RETURN u.userId, u.pagerank ORDER BY u.pagerank DESC LIMIT 10').get_as_df()\n\n\n  \n    \n\n\n\n\n\n\nu.userId\nu.pagerank\n\n\n\n\n0\n599\n0.016401\n\n\n1\n414\n0.014711\n\n\n2\n474\n0.014380\n\n\n3\n448\n0.012942\n\n\n4\n610\n0.008492\n\n\n5\n606\n0.007245\n\n\n6\n274\n0.006253\n\n\n7\n89\n0.006083\n\n\n8\n380\n0.005997\n\n\n9\n318\n0.005920\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nAs a final example, we find the average ratings from a highly influential (i.e, high pagerank score) user and highly influential movies. To filter high pagerank users, we first set a threshold of a user’s score being above or below the top 10 pagerank scores for all users. Any user with a pagerank score above this value is considered an “influential” user. We repeat this same computation to find the threshold for movies. Then we find all ratings between influential users and movies and take their average.\n\nuser_pr_threshold = conn.execute(\n  \"\"\"\n  MATCH (u:User)\n  WITH u.pagerank AS user_pr_threshold\n  ORDER BY u.pagerank DESC LIMIT 10\n  RETURN min(user_pr_threshold)\n  \"\"\"\n).get_as_df().iloc[0,0];\nuser_pr_threshold\n\n0.005919808556361484\n\n\n\nmovie_pr_threshold = conn.execute(\n    \"\"\"\n    MATCH (m:Movie)\n    WITH m.pagerank AS movie_pr_threshold\n    ORDER BY m.pagerank DESC LIMIT 10\n    RETURN min(movie_pr_threshold)\n    \"\"\"\n).get_as_df().iloc[0,0];\nmovie_pr_threshold\n\n0.0007781856282699511\n\n\n\navg_rating_df = conn.execute(\n    \"\"\"\n    MATCH (u:User)-[r:Rating]-&gt;(m:Movie)\n    WHERE u.pagerank &gt; $user_pr_threshold  AND m.pagerank &gt; $movie_pr_threshold\n    RETURN avg(r.rating) as avgRBtwHighPRUserMovies;\n    \"\"\",\n    parameters={\"user_pr_threshold\": user_pr_threshold, \"movie_pr_threshold\": movie_pr_threshold}\n).get_as_df()\navg_rating_df.head()\n\n\n  \n    \n\n\n\n\n\n\navgRBtwHighPRUserMovies\n\n\n\n\n0\n4.239437",
    "crumbs": [
      "Kuzu - Export Query Results to NetworkX"
    ]
  },
  {
    "objectID": "docs/notebooks/KuzuGraphDemo.html",
    "href": "docs/notebooks/KuzuGraphDemo.html",
    "title": "Kuzu Graph Store (LlamaIndex docs)",
    "section": "",
    "text": "This notebook walks through configuring Kùzu to be the backend for graph storage in LlamaIndex.\n\n%%capture\n%pip install llama-index\n%pip install llama-index-llms-openai\n%pip install llama-index-graph-stores-kuzu\n%pip install pyvis\n\n\n# My OpenAI Key\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"\"\n\n\n\n\n# Clean up all the directories used in this notebook\nimport shutil\n\nshutil.rmtree(\"./test1\", ignore_errors=True)\nshutil.rmtree(\"./test2\", ignore_errors=True)\nshutil.rmtree(\"./test3\", ignore_errors=True)\n\n\nimport kuzu\n\ndb = kuzu.Database(\"test1\")\n\n\n\n\n\nfrom llama_index.graph_stores.kuzu import KuzuGraphStore\n\ngraph_store = KuzuGraphStore(db)\n\n\n\n\nfrom llama_index.core import SimpleDirectoryReader, KnowledgeGraphIndex\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\nfrom IPython.display import Markdown, display\nimport kuzu\n\n\n!curl  -LJO https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 75042  100 75042    0     0   276k      0 --:--:-- --:--:-- --:--:--  276k\n\n\n\n!mkdir data\n!mv paul_graham_essay.txt data/\n\n\n!ls -ltra data/\n\ntotal 84\n-rw-r--r-- 1 root root 75042 Aug 21 13:49 paul_graham_essay.txt\ndrwxr-xr-x 1 root root  4096 Aug 21 13:50 ..\ndrwxr-xr-x 2 root root  4096 Aug 21 13:50 .\n\n\n\ndocuments = SimpleDirectoryReader(\n    \"data/\"\n).load_data()\n\n\n# define LLM\n\nllm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\nSettings.llm = llm\nSettings.chunk_size = 512\n\n\nfrom llama_index.core import StorageContext\n\nstorage_context = StorageContext.from_defaults(graph_store=graph_store)\n\n# NOTE: can take a while!\nindex = KnowledgeGraphIndex.from_documents(\n    documents,\n    max_triplets_per_chunk=2,\n    storage_context=storage_context,\n)\n# # To reload from an existing graph store without recomputing each time, use:\n# index = KnowledgeGraphIndex(nodes=[], storage_context=storage_context)\n\n\n\n\nFirst, we can query and send only the triplets to the LLM.\n\nquery_engine = index.as_query_engine(\n    include_text=False, response_mode=\"tree_summarize\"\n)\nresponse = query_engine.query(\n    \"Tell me more about Interleaf\",\n)\n\n\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n\nInterleaf was involved in making software and added a scripting language. Additionally, it was also associated with a reason for existence and eventually faced challenges due to Moore’s law.\n\n\nFor more detailed answers, we can also send the text from where the retrieved tripets were extracted.\n\nquery_engine = index.as_query_engine(\n    include_text=True, response_mode=\"tree_summarize\"\n)\nresponse = query_engine.query(\n    \"Tell me more about Interleaf\",\n)\n\n\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n\nInterleaf was a company that made software for creating documents. They added a scripting language inspired by Emacs, which was a dialect of Lisp. The software they created had a specific purpose, which was to allow users to build their own online stores. Despite their impressive technology and smart employees, Interleaf ultimately faced challenges and was impacted by Moore’s Law, leading to its eventual decline.\n\n\n\n\n\n\n# NOTE: can take a while!\ndb = kuzu.Database(\"test2\")\ngraph_store = KuzuGraphStore(db)\nstorage_context = StorageContext.from_defaults(graph_store=graph_store)\nnew_index = KnowledgeGraphIndex.from_documents(\n    documents,\n    max_triplets_per_chunk=2,\n    storage_context=storage_context,\n    include_embeddings=True,\n)\n\n\n# query using top 3 triplets plus keywords (duplicate triplets are removed)\nquery_engine = index.as_query_engine(\n    include_text=True,\n    response_mode=\"tree_summarize\",\n    embedding_mode=\"hybrid\",\n    similarity_top_k=5,\n)\nresponse = query_engine.query(\n    \"Tell me more about what the author worked on at Interleaf\",\n)\n\n\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n\nThe author worked at Interleaf, a company that made software for creating documents. Inspired by Emacs, Interleaf added a scripting language that was a dialect of Lisp. The author was hired as a Lisp hacker to write things in this scripting language. However, the author found it challenging to work with the software at Interleaf due to his lack of understanding of C and his reluctance to learn it. Despite this, the author managed to learn some valuable lessons at Interleaf, mostly about what not to do.\n\n\n\n\n\n\n## create graph\nfrom pyvis.network import Network\nfrom IPython.core.display import display, HTML\n\ng = index.get_networkx_graph()\nnet = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\nnet.from_nx(g)\nnet.show(\"kuzugraph_draw.html\")\ndisplay(HTML('kuzugraph_draw.html'))\n\nkuzugraph_draw.html\n\n\n\n    \n        \n        \n            \n            \n            \n            \n        \n\n\n\n\n\n        \n        \n\n\n        \n          \n        \n        \n    \n\n\n    \n        \n            \n            \n            \n        \n\n        \n            \n              \n                0%\n                \n                  \n                \n              \n            \n        \n        \n\n        \n    \n\n\n\n\n\n\n\nfrom llama_index.core.node_parser import SentenceSplitter\n\n\nnode_parser = SentenceSplitter()\n\n\nnodes = node_parser.get_nodes_from_documents(documents)\n\n\n# initialize an empty database\ndb = kuzu.Database(\"test3\")\ngraph_store = KuzuGraphStore(db)\nstorage_context = StorageContext.from_defaults(graph_store=graph_store)\nindex = KnowledgeGraphIndex(\n    [],\n    storage_context=storage_context,\n)\n\n\n# add keyword mappings and nodes manually\n# add triplets (subject, relationship, object)\n\n# for node 0\nnode_0_tups = [\n    (\"author\", \"worked on\", \"writing\"),\n    (\"author\", \"worked on\", \"programming\"),\n]\nfor tup in node_0_tups:\n    index.upsert_triplet_and_node(tup, nodes[0])\n\n# for node 1\nnode_1_tups = [\n    (\"Interleaf\", \"made software for\", \"creating documents\"),\n    (\"Interleaf\", \"added\", \"scripting language\"),\n    (\"software\", \"generate\", \"web sites\"),\n]\nfor tup in node_1_tups:\n    index.upsert_triplet_and_node(tup, nodes[1])\n\n\nquery_engine = index.as_query_engine(\n    include_text=False, response_mode=\"tree_summarize\"\n)\nresponse = query_engine.query(\n    \"Tell me more about Interleaf\",\n)\n\n\nstr(response)\n\n'Interleaf was involved in creating documents and also added a scripting language to its software.'",
    "crumbs": [
      "Kuzu Graph Store (LlamaIndex docs)"
    ]
  },
  {
    "objectID": "docs/notebooks/KuzuGraphDemo.html#prepare-for-kùzu",
    "href": "docs/notebooks/KuzuGraphDemo.html#prepare-for-kùzu",
    "title": "Kuzu Graph Store (LlamaIndex docs)",
    "section": "",
    "text": "# Clean up all the directories used in this notebook\nimport shutil\n\nshutil.rmtree(\"./test1\", ignore_errors=True)\nshutil.rmtree(\"./test2\", ignore_errors=True)\nshutil.rmtree(\"./test3\", ignore_errors=True)\n\n\nimport kuzu\n\ndb = kuzu.Database(\"test1\")",
    "crumbs": [
      "Kuzu Graph Store (LlamaIndex docs)"
    ]
  },
  {
    "objectID": "docs/notebooks/KuzuGraphDemo.html#using-knowledge-graph-with-kuzugraphstore",
    "href": "docs/notebooks/KuzuGraphDemo.html#using-knowledge-graph-with-kuzugraphstore",
    "title": "Kuzu Graph Store (LlamaIndex docs)",
    "section": "",
    "text": "from llama_index.graph_stores.kuzu import KuzuGraphStore\n\ngraph_store = KuzuGraphStore(db)\n\n\n\n\nfrom llama_index.core import SimpleDirectoryReader, KnowledgeGraphIndex\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\nfrom IPython.display import Markdown, display\nimport kuzu\n\n\n!curl  -LJO https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 75042  100 75042    0     0   276k      0 --:--:-- --:--:-- --:--:--  276k\n\n\n\n!mkdir data\n!mv paul_graham_essay.txt data/\n\n\n!ls -ltra data/\n\ntotal 84\n-rw-r--r-- 1 root root 75042 Aug 21 13:49 paul_graham_essay.txt\ndrwxr-xr-x 1 root root  4096 Aug 21 13:50 ..\ndrwxr-xr-x 2 root root  4096 Aug 21 13:50 .\n\n\n\ndocuments = SimpleDirectoryReader(\n    \"data/\"\n).load_data()\n\n\n# define LLM\n\nllm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\nSettings.llm = llm\nSettings.chunk_size = 512\n\n\nfrom llama_index.core import StorageContext\n\nstorage_context = StorageContext.from_defaults(graph_store=graph_store)\n\n# NOTE: can take a while!\nindex = KnowledgeGraphIndex.from_documents(\n    documents,\n    max_triplets_per_chunk=2,\n    storage_context=storage_context,\n)\n# # To reload from an existing graph store without recomputing each time, use:\n# index = KnowledgeGraphIndex(nodes=[], storage_context=storage_context)\n\n\n\n\nFirst, we can query and send only the triplets to the LLM.\n\nquery_engine = index.as_query_engine(\n    include_text=False, response_mode=\"tree_summarize\"\n)\nresponse = query_engine.query(\n    \"Tell me more about Interleaf\",\n)\n\n\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n\nInterleaf was involved in making software and added a scripting language. Additionally, it was also associated with a reason for existence and eventually faced challenges due to Moore’s law.\n\n\nFor more detailed answers, we can also send the text from where the retrieved tripets were extracted.\n\nquery_engine = index.as_query_engine(\n    include_text=True, response_mode=\"tree_summarize\"\n)\nresponse = query_engine.query(\n    \"Tell me more about Interleaf\",\n)\n\n\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n\nInterleaf was a company that made software for creating documents. They added a scripting language inspired by Emacs, which was a dialect of Lisp. The software they created had a specific purpose, which was to allow users to build their own online stores. Despite their impressive technology and smart employees, Interleaf ultimately faced challenges and was impacted by Moore’s Law, leading to its eventual decline.\n\n\n\n\n\n\n# NOTE: can take a while!\ndb = kuzu.Database(\"test2\")\ngraph_store = KuzuGraphStore(db)\nstorage_context = StorageContext.from_defaults(graph_store=graph_store)\nnew_index = KnowledgeGraphIndex.from_documents(\n    documents,\n    max_triplets_per_chunk=2,\n    storage_context=storage_context,\n    include_embeddings=True,\n)\n\n\n# query using top 3 triplets plus keywords (duplicate triplets are removed)\nquery_engine = index.as_query_engine(\n    include_text=True,\n    response_mode=\"tree_summarize\",\n    embedding_mode=\"hybrid\",\n    similarity_top_k=5,\n)\nresponse = query_engine.query(\n    \"Tell me more about what the author worked on at Interleaf\",\n)\n\n\ndisplay(Markdown(f\"&lt;b&gt;{response}&lt;/b&gt;\"))\n\nThe author worked at Interleaf, a company that made software for creating documents. Inspired by Emacs, Interleaf added a scripting language that was a dialect of Lisp. The author was hired as a Lisp hacker to write things in this scripting language. However, the author found it challenging to work with the software at Interleaf due to his lack of understanding of C and his reluctance to learn it. Despite this, the author managed to learn some valuable lessons at Interleaf, mostly about what not to do.\n\n\n\n\n\n\n## create graph\nfrom pyvis.network import Network\nfrom IPython.core.display import display, HTML\n\ng = index.get_networkx_graph()\nnet = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\nnet.from_nx(g)\nnet.show(\"kuzugraph_draw.html\")\ndisplay(HTML('kuzugraph_draw.html'))\n\nkuzugraph_draw.html\n\n\n\n    \n        \n        \n            \n            \n            \n            \n        \n\n\n\n\n\n        \n        \n\n\n        \n          \n        \n        \n    \n\n\n    \n        \n            \n            \n            \n        \n\n        \n            \n              \n                0%\n                \n                  \n                \n              \n            \n        \n        \n\n        \n    \n\n\n\n\n\n\n\nfrom llama_index.core.node_parser import SentenceSplitter\n\n\nnode_parser = SentenceSplitter()\n\n\nnodes = node_parser.get_nodes_from_documents(documents)\n\n\n# initialize an empty database\ndb = kuzu.Database(\"test3\")\ngraph_store = KuzuGraphStore(db)\nstorage_context = StorageContext.from_defaults(graph_store=graph_store)\nindex = KnowledgeGraphIndex(\n    [],\n    storage_context=storage_context,\n)\n\n\n# add keyword mappings and nodes manually\n# add triplets (subject, relationship, object)\n\n# for node 0\nnode_0_tups = [\n    (\"author\", \"worked on\", \"writing\"),\n    (\"author\", \"worked on\", \"programming\"),\n]\nfor tup in node_0_tups:\n    index.upsert_triplet_and_node(tup, nodes[0])\n\n# for node 1\nnode_1_tups = [\n    (\"Interleaf\", \"made software for\", \"creating documents\"),\n    (\"Interleaf\", \"added\", \"scripting language\"),\n    (\"software\", \"generate\", \"web sites\"),\n]\nfor tup in node_1_tups:\n    index.upsert_triplet_and_node(tup, nodes[1])\n\n\nquery_engine = index.as_query_engine(\n    include_text=False, response_mode=\"tree_summarize\"\n)\nresponse = query_engine.query(\n    \"Tell me more about Interleaf\",\n)\n\n\nstr(response)\n\n'Interleaf was involved in creating documents and also added a scripting language to its software.'",
    "crumbs": [
      "Kuzu Graph Store (LlamaIndex docs)"
    ]
  },
  {
    "objectID": "docs/demo/kuzu-use-repl.html",
    "href": "docs/demo/kuzu-use-repl.html",
    "title": "kuzu / pyvis leverage jupyterlite repl",
    "section": "",
    "text": "Try our experimental JupyterLite console with Kuzu!",
    "crumbs": [
      "kuzu / pyvis leverage jupyterlite repl"
    ]
  },
  {
    "objectID": "docs/notebooks/mesonetwork.html",
    "href": "docs/notebooks/mesonetwork.html",
    "title": "Réseau entre les citations des avis du CCNE",
    "section": "",
    "text": "Si l’application ne fonctionne pas directement sur ce site, veuillez utiliser :\nRéseau entre les citations, en application standalone",
    "crumbs": [
      "Réseau entre les citations des avis du CCNE"
    ]
  },
  {
    "objectID": "docs/notebooks/mesonetwork.html#sélectionnez-un-président-pour-avoir-les-avis-publiés-sous-sa-présidence",
    "href": "docs/notebooks/mesonetwork.html#sélectionnez-un-président-pour-avoir-les-avis-publiés-sous-sa-présidence",
    "title": "Réseau entre les citations des avis du CCNE",
    "section": "Sélectionnez un président pour avoir les avis publiés sous sa présidence",
    "text": "Sélectionnez un président pour avoir les avis publiés sous sa présidence\nQuand le choix est changé, l’intervalle des avis séléctionné est mis à jour.\n\nviewof president = Inputs.radio([\"Bernard (83-91)\", \"Changeux (92-99)\", \"Sicard (00-08)\", \"Grimfeld (09-11)\", \"Ameisen (12-15)\", \"Delfraissy (16-)\"], {\n  label: \"Présent\",\n  value: \"Delfraissy (16-)\"\n})",
    "crumbs": [
      "Réseau entre les citations des avis du CCNE"
    ]
  },
  {
    "objectID": "docs/notebooks/mesonetwork.html#sélectionner-lintervalle-des-avis-à-afficher-par-numéro-de-publication-dans-lordre-chronologique",
    "href": "docs/notebooks/mesonetwork.html#sélectionner-lintervalle-des-avis-à-afficher-par-numéro-de-publication-dans-lordre-chronologique",
    "title": "Réseau entre les citations des avis du CCNE",
    "section": "Sélectionner l’intervalle des avis à afficher (par numéro de publication, dans l’ordre chronologique):",
    "text": "Sélectionner l’intervalle des avis à afficher (par numéro de publication, dans l’ordre chronologique):\n\npresident_data = await d3.json(\"data/max_min_par_president.json\");\n\n// Trouver la plage correspondant au président sélectionné\nselected_president_data = president_data.find(d =&gt; d.president === president);\n\n// Définir la plage des avis en fonction du président sélectionné\nimport {interval} from '@mootari/range-slider'\nviewof avis_range = interval([1, 144], {\n  step: 1,\n  value: [selected_president_data.avis_min, selected_president_data.avis_max],\n  label: \"Plage avis\",\n})",
    "crumbs": [
      "Réseau entre les citations des avis du CCNE"
    ]
  },
  {
    "objectID": "docs/notebooks/mesonetwork.html#choisir-les-catégories-à-afficher",
    "href": "docs/notebooks/mesonetwork.html#choisir-les-catégories-à-afficher",
    "title": "Réseau entre les citations des avis du CCNE",
    "section": "Choisir les catégories à afficher",
    "text": "Choisir les catégories à afficher\n\nlist_all_categories = [\"Auteurs\",\"Autorités\",\"CCNE\",\"Comité d'éthique\",\"Comparaison pays\",\"Etat\",\"Forums\",\"Loi\",\"Org Internationales\",\"Presse\", \"Science, littérature\",\"Société\"];\nviewof categories = Inputs.checkbox(\n  list_all_categories, \n  {label: \"Catégories\", value: [\"Loi\", \"CCNE\"]}\n);",
    "crumbs": [
      "Réseau entre les citations des avis du CCNE"
    ]
  },
  {
    "objectID": "docs/notebooks/mesonetwork.html#afficher-ou-cacher-le-réseau",
    "href": "docs/notebooks/mesonetwork.html#afficher-ou-cacher-le-réseau",
    "title": "Réseau entre les citations des avis du CCNE",
    "section": "Afficher ou cacher le réseau",
    "text": "Afficher ou cacher le réseau\n\n// Afficher ou cacher le réseau\nviewof toggle_network = Inputs.toggle({label: \"Afficher le réseau\", value: true});",
    "crumbs": [
      "Réseau entre les citations des avis du CCNE"
    ]
  }
]